#!/usr/bin/env python

import sys, os
import argparse
import json

from sklearn.cross_validation import train_test_split
from keras import layers
from keras.callbacks import History
from keras import optimizers
from keras.objectives import binary_crossentropy

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from data.utils import load_sparse_csr
from analysis import visualization, evaluate
from models import FullConnected, Convolution

BATCH_SIZE = 15

def parse_optimizer(args):
	"""
		Parses the optimizer to use
	"""
	if args.optimizer == 'adam':
		return optimizers.Adam(args.learningrate)
	elif args.optimizer == 'sgd':
		return optimizers.SGD(args.learningrate)
	else:
		raise ValueError('Unknown optimizer')
	

def load_data(args):
	"""
		Loads the dataset according to the arguments
	"""
	embedding = args.embedding
	FOLDER = None

	if 'tfidf' in embedding or 'counts' in embedding:
		ngram1 = args.ngram_min
		ngram2 = args.ngram_max
		vocabsize = args.vocabsize
		if vocabsize not in [5000, 10000]:
			raise ValueError('Unknown vocab size')
		FOLDER = './data/processed/tfidf_'+str(vocabsize)+'_'+str(ngram1)+'-'+str(ngram2)+'grams'

		print('loading data from:' + FOLDER)

		X = load_sparse_csr(FOLDER + '/X_train.csr.npz').toarray()
		Y = np.load(FOLDER + '/Y_train.npy')


	elif 'glove' in embedding or 'learned' in embedding:
		embeddingsize = args.embeddingsize
		vocabsize = args.vocabsize
		if vocabsize not in [5000, 9000, None, 500]:
			raise ValueError('Unknown vocab size')
		if embeddingsize not in [100, 50]:
			raise ValueError('Unknown Embedding size')
		FOLDER = './data/processed/glove'+str(embeddingsize)+'_top'+str(vocabsize)

		print('Loading data from: ' + FOLDER)
		X = np.load(FOLDER + '/X_train.npy')
		Y = np.load(FOLDER + '/Y_train.npy')
		
	else:
		raise IOError('Unknown embedding ' + embedding + ' supplied.')

	return train_test_split(X, Y, test_size=0.1), FOLDER

def main(args):
	"""
		Loading data
	"""
	params = json.load(open(args.params, 'r')) # information about architecture
	(X, X_val, Y, Y_val), folder = load_data(args) # dataset to use
	
	if args.subset:
		X = X[:args.subset, :]
		Y = Y[:args.subset]

	FEATURE_SIZE = X.shape[1]

	"""
		Model building
	"""
	model = None
	if args.embedding not in ['glove', 'learned']:
		if args.architecture == 'FNN':
			model = FullConnected.FC(FEATURE_SIZE, **params)
		elif args.architecture == 'CNN':
			model = Convolution.CNN(FEATURE_SIZE, **params)
		else:
			raise ValueError('Unsupported architecture ' + args.architecture)
	else:
		"""
			Glove Embeddings!
		"""
		embedding = None
		
		if args.embedding == 'learned':
			LEN_WORD_MAP = np.max(np.vstack([X, X_val ])) #maximimum embedding size
			embedding = layers.Embedding(LEN_WORD_MAP + 1, args.embeddingsize, input_length=FEATURE_SIZE)

		elif args.embedding == 'glove':
			embedding_matrix = np.load(folder + '/embedding_matrix.npy')
			LEN_WORD_MAP = embedding_matrix.shape[0]
			EMBED_SIZE = embedding_matrix.shape[1]

			embedding = layers.Embedding( LEN_WORD_MAP, EMBED_SIZE, \
				input_length=FEATURE_SIZE, weights=[embedding_matrix], \
				trainable=args.embedding_trainable)

		else:
			raise ValueError('Unsupported embedding')

		if args.architecture == 'FNN':
			model = FullConnected.FC_embedding(FEATURE_SIZE, embedding=embedding, **params)
		elif args.architecture == 'CNN':
			model = Convolution.CNN_embedding(FEATURE_SIZE, embedding=embedding, **params)
		else:
			raise ValueError('Unsupported architecture ' + args.architecture)
	# add output layers.
	model.add( layers.Dense(1) )
	model.add( layers.Activation('sigmoid') )
	
	optimizer = parse_optimizer(args)

	model.compile(optimizer=optimizer, loss=binary_crossentropy, metrics=['accuracy'])

	"""
		Training
	"""
	history = History()
	try:
		model.fit(X, Y, batch_size=BATCH_SIZE, shuffle=True, \
			validation_split=0.1, verbose=1, nb_epoch=args.epochs, callbacks=[history])

	except KeyboardInterrupt as e:
		print('Training Stopped Early')

	"""
		Evaluations
	"""
	print(args)
	Y_train_preds = model.predict(X, verbose=False)
	Y_val_preds = model.predict(X_val, verbose=False)

	evaluate.evaluate(Y, Y_train_preds, title='Training')
	evaluate.evaluate(Y_val, Y_val_preds, title='Validation')
	f1_, _ = visualization.training_plot(history, args.outfile, metric='loss')
	f2_, _ = visualization.training_plot(history, args.outfile, metric='acc')

	fpr_train, tpr_train, auc_train = evaluate.roc_auc(Y, Y_train_preds)
	fpr_val, tpr_val, auc_val = evaluate.roc_auc(Y_val, Y_val_preds)

	f3_, _ = visualization.plot_auc([fpr_train, fpr_val], \
									[tpr_train, tpr_val], \
									[auc_train, auc_val], \
									labels=['Training Set', 'Validation Set'])

	plt.show()
	f1_.savefig('loss.pdf')
	f2_.savefig('acc.pdf')
	f3_.savefig('auc.pdf')


if __name__ == '__main__':
	parser = argparse.ArgumentParser()
	named_args = parser.add_argument_group('named arguments')

	named_args.add_argument('-e', '--embedding', metavar='|',
						help="""Representation of the data
						one of: 'glove', 'learned', 'tfidf', 'counts'""",
						required=True)

	named_args.add_argument('-o', '--outfile', metavar='|',
						help='File to store trained model',
						required=False)

	named_args.add_argument('-n', '--epochs', metavar='|',
						help='Number of epochs to train network',
						required=True, type=int)

	named_args.add_argument('-a', '--architecture', metavar='|',
						help='Network Architecture to use (FNN, CNN, LSTM)',
						required=True)

	named_args.add_argument('-p', '--params', metavar='|',
						help='Parameter file to use to build the architecture',
						required=True)

	named_args.add_argument('-t', '--optimizer', metavar='|',
						help='Optimizer to use',
						required=False, default='adam')

	named_args.add_argument('-lr', '--learningrate', metavar='|',
						help='Optimizer to use',
						required=False, default=0.01)

	named_args = parser.add_argument_group('embedding arguments')
	named_args.add_argument('-v', '--vocabsize', metavar='|',
						help="""Number of words in vocabulary""",
						required=False, type=int, default=5000)

	named_args.add_argument('-s', '--embeddingsize', metavar='|',
						help="""Size of embedding dimension""",
						required=False, type=int, default=50)

	named_args.add_argument('-nmi', '--ngram-min', metavar='|',
						help="""Minmum n-gram size""",
						required=False, type=int, default=1)

	named_args.add_argument('-nmx', '--ngram-max', metavar='|',
						help="""Maximum ngram size""",
						required=False, type=int, default=1)

	named_args.add_argument('-et', '--embedding-trainable', metavar='|',
						help="""Defines if the embedding is trainable""",
						required=False, type=bool, default=True)

	named_args.add_argument('-ss', '--subset', metavar='|',
						help="""Uses a subset of data to do training""",
						required=False, type=int, default=10000)

	args = parser.parse_args()
	main(args)
